{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing essential libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas-profiling in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (3.1.0)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (2.11.2)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.1.12)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (1.18.5)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (5.3.1)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (1.9.0)\n",
      "Requirement already satisfied: markupsafe~=2.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.24.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (2.24.0)\n",
      "Requirement already satisfied: missingno>=0.4.2 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.5.1)\n",
      "Requirement already satisfied: phik>=0.11.1 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.12.2)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.4 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.7.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (1.5.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (3.2.2)\n",
      "Requirement already satisfied: joblib~=1.0.1 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (1.0.1)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (1.0.5)\n",
      "Requirement already satisfied: seaborn>=0.10.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (0.11.2)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.1.0 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.1.0)\n",
      "Requirement already satisfied: multimethod>=1.4 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (1.8)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pydantic>=1.8.1->pandas-profiling) (4.1.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (2020.6.20)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (19.3.0)\n",
      "Requirement already satisfied: networkx>=2.4 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (2.4)\n",
      "Requirement already satisfied: imagehash; extra == \"type_image_path\" in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (4.2.1)\n",
      "Requirement already satisfied: Pillow; extra == \"type_image_path\" in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2020.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from tqdm>=4.48.2->pandas-profiling) (0.4.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from networkx>=2.4->visions[type_image_path]==0.7.4->pandas-profiling) (4.4.2)\n",
      "Requirement already satisfied: PyWavelets in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.7.4->pandas-profiling) (1.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.7.4->pandas-profiling) (1.15.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (5.7.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from plotly) (1.15.0)\n",
      "Requirement already satisfied: imblearn in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from imblearn) (0.9.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.18.5)\n",
      "Requirement already satisfied: catboost in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: plotly in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (5.7.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (0.19.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (1.18.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (3.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (1.5.0)\n",
      "Requirement already satisfied: six in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2020.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas-profiling\n",
    "!{sys.executable} -m pip install plotly\n",
    "!{sys.executable} -m pip  install imblearn\n",
    "!{sys.executable} -m pip  install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing various models and tools \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the dataset that has been cleaned and does not contain any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.66         0.00             1.8      0.075   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 13.0                  40.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        0  \n",
       "1      9.8        0  \n",
       "2      9.8        0  \n",
       "3      9.8        1  \n",
       "4      9.4        0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data = pd.read_csv(\"binned_no_dup.csv\")\n",
    "\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "We can remove the outliers for each predictor variable in our dataset, so that our trained models will not be impacted by the outliers. We do this by removing data points with predictor variable values beyond 1.5 * IQR of the first or third quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>1019.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.169872</td>\n",
       "      <td>0.522507</td>\n",
       "      <td>0.250805</td>\n",
       "      <td>2.198822</td>\n",
       "      <td>0.078452</td>\n",
       "      <td>14.959764</td>\n",
       "      <td>42.388616</td>\n",
       "      <td>0.996547</td>\n",
       "      <td>3.322385</td>\n",
       "      <td>0.631237</td>\n",
       "      <td>10.385967</td>\n",
       "      <td>0.538763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.475073</td>\n",
       "      <td>0.167364</td>\n",
       "      <td>0.182313</td>\n",
       "      <td>0.452214</td>\n",
       "      <td>0.014973</td>\n",
       "      <td>8.838951</td>\n",
       "      <td>26.625802</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.132589</td>\n",
       "      <td>0.114861</td>\n",
       "      <td>0.992372</td>\n",
       "      <td>0.498740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.992350</td>\n",
       "      <td>2.940000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995500</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.996560</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0.997600</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.300000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>3.680000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1019.000000       1019.000000  1019.000000     1019.000000   \n",
       "mean        8.169872          0.522507     0.250805        2.198822   \n",
       "std         1.475073          0.167364     0.182313        0.452214   \n",
       "min         5.100000          0.120000     0.000000        1.200000   \n",
       "25%         7.100000          0.390000     0.080000        1.900000   \n",
       "50%         7.800000          0.520000     0.240000        2.100000   \n",
       "75%         9.000000          0.630000     0.400000        2.500000   \n",
       "max        12.300000          1.010000     0.730000        3.650000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1019.000000          1019.000000           1019.000000  1019.000000   \n",
       "mean      0.078452            14.959764             42.388616     0.996547   \n",
       "std       0.014973             8.838951             26.625802     0.001616   \n",
       "min       0.039000             1.000000              6.000000     0.992350   \n",
       "25%       0.069000             8.000000             22.000000     0.995500   \n",
       "50%       0.078000            13.000000             36.000000     0.996560   \n",
       "75%       0.087000            20.000000             56.000000     0.997600   \n",
       "max       0.122000            42.000000            124.000000     1.001000   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1019.000000  1019.000000  1019.000000  1019.000000  \n",
       "mean      3.322385     0.631237    10.385967     0.538763  \n",
       "std       0.132589     0.114861     0.992372     0.498740  \n",
       "min       2.940000     0.330000     8.700000     0.000000  \n",
       "25%       3.230000     0.550000     9.500000     0.000000  \n",
       "50%       3.320000     0.610000    10.100000     1.000000  \n",
       "75%       3.400000     0.700000    11.000000     1.000000  \n",
       "max       3.680000     0.980000    13.400000     1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning dataset: Removing outliers\n",
    "wine_predictors = wine_data.drop('quality', axis = 1)\n",
    "wine_quality = wine_data['quality']\n",
    "\n",
    "\n",
    "Q1_joint = wine_predictors.quantile(0.25)\n",
    "Q3_joint = wine_predictors.quantile(0.75)\n",
    "IQR_joint = Q3_joint - Q1_joint\n",
    "idx_joint_cleaned = ~((wine_predictors < (Q1_joint - 1.5 * IQR_joint)) | (wine_predictors > (Q3_joint + 1.5 * IQR_joint))).any(axis=1)\n",
    "\n",
    "wine_predictors_cleaned = wine_predictors.loc[idx_joint_cleaned]\n",
    "\n",
    "wine_data_cleaned = wine_predictors_cleaned.join(wine_quality)\n",
    "\n",
    "wine_data_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1aaa7f44430>,\n",
       "  <matplotlib.axis.XTick at 0x1aaa7f44790>],\n",
       " <a list of 2 Text major ticklabel objects>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x = wine_data_cleaned['quality'].value_counts().index.tolist(),\n",
    "       height = wine_data_cleaned['quality'].value_counts())\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Quality')\n",
    "plt.xticks([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in `2.3 Modelling with Feature Selection`, the median, min, max, 1st quartile, and 3rd quartile values for pH and residual sugar, do not vary significantly for qualities good and bad. This suggests there is no strong relationship between these 2 variables and the quality of the wine.\n",
    "\n",
    "Hence, we decide to drop these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1019 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  chlorides  \\\n",
       "0               7.4             0.700         0.00      0.076   \n",
       "1               7.8             0.880         0.00      0.098   \n",
       "2               7.8             0.760         0.04      0.092   \n",
       "3              11.2             0.280         0.56      0.075   \n",
       "4               7.4             0.660         0.00      0.075   \n",
       "...             ...               ...          ...        ...   \n",
       "1354            6.8             0.620         0.08      0.068   \n",
       "1355            6.2             0.600         0.08      0.090   \n",
       "1356            5.9             0.550         0.10      0.062   \n",
       "1357            5.9             0.645         0.12      0.075   \n",
       "1358            6.0             0.310         0.47      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density  sulphates  alcohol  \\\n",
       "0                    11.0                  34.0  0.99780       0.56      9.4   \n",
       "1                    25.0                  67.0  0.99680       0.68      9.8   \n",
       "2                    15.0                  54.0  0.99700       0.65      9.8   \n",
       "3                    17.0                  60.0  0.99800       0.58      9.8   \n",
       "4                    13.0                  40.0  0.99780       0.56      9.4   \n",
       "...                   ...                   ...      ...        ...      ...   \n",
       "1354                 28.0                  38.0  0.99651       0.82      9.5   \n",
       "1355                 32.0                  44.0  0.99490       0.58     10.5   \n",
       "1356                 39.0                  51.0  0.99512       0.76     11.2   \n",
       "1357                 32.0                  44.0  0.99547       0.71     10.2   \n",
       "1358                 18.0                  42.0  0.99549       0.66     11.0   \n",
       "\n",
       "      quality  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  \n",
       "...       ...  \n",
       "1354        1  \n",
       "1355        0  \n",
       "1356        1  \n",
       "1357        0  \n",
       "1358        1  \n",
       "\n",
       "[1019 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data_dropped = wine_data_cleaned.drop(columns=['pH', 'residual sugar'])\n",
    "\n",
    "wine_data_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Machine Learning</h1>\n",
    "\n",
    "We split the dataset into the predictors (X) and the response variable 'quality' (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = 'quality'\n",
    "X = wine_data_dropped.drop([target_label], axis=1)\n",
    "y = wine_data_dropped[target_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1aaa90f4b20>,\n",
       "  <matplotlib.axis.XTick at 0x1aaa90f4af0>],\n",
       " <a list of 2 Text major ticklabel objects>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x = y.value_counts().index.tolist(),\n",
    "       height = y.value_counts())\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Quality')\n",
    "plt.xticks([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train Test Split </h2>\n",
    "\n",
    "We use the train_test_split() function to split the dataset into the train set, and the test set, with a test size of 0.2.\n",
    "We also specify stratify = y. This is to ensure that the ratio of good:bad values are consistent throughout the train set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Fitting & Hyperparameter Tuning</h2>\n",
    "\n",
    "For each of the models used below, the GridSearchCV function is used to determine the optimal hyperparameters for each model (i.e. the hyperparameters which result in the highest accuracy scores on the trainset). 5-fold cross validation is used to obtain unbiased results as well as to determine the optimal hyperparameters of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries used for model fitting\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> (1) Logistic Regression </H2>\n",
    "\n",
    "Logistic regression is a process of modeling the probability of a discrete outcome given an input variable. Logistic regression is a useful analysis method for classification problems, where you are trying to determine if a new sample fits best into a category. \n",
    "It is a simple and more efficient method for binary and linear classification problems. It performs relatively well in our dataset as our response variables are binary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 1.0, 'penalty': 'l2', 'random_state': 42, 'solver': 'newton-cg'}\n",
      "Best CV score:  0.7374233128834355\n",
      "Test accuracy:  0.7401960784313726\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'penalty':['l1', 'l2', 'elasticnet'],\n",
    "              'C': np.logspace(-3, 3, 7),\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "             'random_state': [42]}\n",
    "\n",
    "grid_search_logreg = GridSearchCV(LogisticRegression(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=1)\n",
    "grid_search_logreg.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_logreg.best_params_)\n",
    "print(\"Best CV score: \", grid_search_logreg.best_score_)\n",
    "\n",
    "best_logreg = LogisticRegression(**grid_search_logreg.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (2) Gaussian Naive Bayes </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. \n",
    "\n",
    "For our dataset, we chose to use Gaussian Naive Bayes where the likelihood of each predictor is assumed to be Gaussian (normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'var_smoothing': 2.310129700083158e-09}\n",
      "Best CV score:  0.7276073619631902\n",
      "Test accuracy:  0.7598039215686274\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'var_smoothing': np.logspace(0,-9, num=100),}\n",
    "\n",
    "grid_search_gnb = GridSearchCV(GaussianNB(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "grid_search_gnb.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_gnb.best_params_)\n",
    "print(\"Best CV score: \", grid_search_gnb.best_score_)\n",
    "\n",
    "best_gnb = GaussianNB(**grid_search_gnb.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_gnb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (3) Decision Tree Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree Classifier belongs to the family of supervised learning algorithms. The intuition behind Decision Trees is that you use the dataset features to create yes/no questions and continually split the dataset until you isolate all data points belonging to each class. The Decision Tree classifier can handle both categorical and numerical data (numerical data as predictors in this dataset), and is simple to understand and interpret since the trees can be visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': None, 'random_state': 42}\n",
      "Best CV score:  0.6871165644171779\n",
      "Test accuracy:  0.6568627450980392\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth':[None, [i for i in range(1,50,1)]],\n",
    "             'random_state': [42]}\n",
    "\n",
    "grid_search_dtc = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_dtc.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_dtc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_dtc.best_score_)\n",
    "\n",
    "best_dtc = DecisionTreeClassifier(**grid_search_dtc.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_dtc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (4) Random Forest Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier builds multiple decision trees and merges them together to get a more accurate and stable prediction. One big advantage of random forest is that it can be used for both classification and regression problems, and is suitable for binary classification of wines in this dataset.  Random forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in an improved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 13, 'n_estimators': 50, 'random_state': 42}\n",
      "Best CV score:  0.747239263803681\n",
      "Test accuracy:  0.7401960784313726\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth':[i for i in range(1,30,3)],\n",
    "             'n_estimators': [j for j in range(10, 601, 40)],\n",
    "             'random_state': [42]}\n",
    "\n",
    "\n",
    "grid_search_rfc = GridSearchCV(RandomForestClassifier(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_rfc.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_rfc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_rfc.best_score_)\n",
    "\n",
    "best_rfc = RandomForestClassifier(**grid_search_rfc.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (5) AdaBoost Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get a high accuracy strong classifier. The method automatically adjusts its parameters to the data based on the actual performance in the current iteration, meaning, both the weights for re-weighting the data and the weights for the final aggregation are re-computed iteratively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.1, 'n_estimators': 210, 'random_state': 42}\n",
      "Best CV score:  0.7435582822085889\n",
      "Test accuracy:  0.7254901960784313\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'learning_rate':[0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "             'n_estimators': [j for j in range(10, 601, 40)],\n",
    "             'random_state': [42]}\n",
    "\n",
    "grid_search_abc = GridSearchCV(AdaBoostClassifier(), param_grid, cv= KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_abc.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_abc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_abc.best_score_)\n",
    "\n",
    "best_abc = AdaBoostClassifier(**grid_search_abc.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_abc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (6) CatBoost Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CatBoost Classifier is a high-performance open source library for gradient boosting on decision trees. In the growing procedure of the decision trees, CatBoost does not follow similar gradient boosting models. Instead, CatBoost grows oblivious trees, which means that the trees are grown by imposing the rule that all nodes at the same level, test the same predictor with the same condition, and hence an index of a leaf can be calculated with bitwise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.2, 'n_estimators': 450, 'random_state': 42}\n",
      "Best CV score:  0.7374233128834357\n",
      "Test accuracy:  0.7450980392156863\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "param_grid = {'learning_rate': [0.08, 0.1, 0.15, 0.2, 0.25, 0.3], \n",
    "             'n_estimators': [200, 250, 300, 350, 400, 450, 500],\n",
    "             'random_state': [42]}\n",
    "\n",
    "\n",
    "grid_search_cbc = GridSearchCV(CatBoostClassifier(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_cbc.fit(X_train, y_train, silent=True)\n",
    "print(\"Best parameters: \", grid_search_cbc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_cbc.best_score_)\n",
    "\n",
    "best_cbc = CatBoostClassifier(**grid_search_cbc.best_params_).fit(X_train, y_train, silent=True)\n",
    "print(\"Test accuracy: \", best_cbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (7) GradientBoost Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifiers are the AdaBoosting method combined with weighted minimization, after which the classifiers and weighted inputs are recalculated. The objective of Gradient Boosting Classifiers is to minimize the loss, or the difference between the actual class value of the training example and the predicted class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.008, 'n_estimators': 500, 'random_state': 42}\n",
      "Best CV score:  0.7361963190184049\n",
      "Test accuracy:  0.7450980392156863\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'learning_rate':[0.008, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24], \n",
    "             'n_estimators': [100, 150, 200, 250, 300, 350, 400, 500, 550, 600],\n",
    "             'random_state': [42]}\n",
    "\n",
    "grid_search_gbc = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_gbc.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_gbc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_gbc.best_score_)\n",
    "\n",
    "best_gbc = GradientBoostingClassifier(**grid_search_gbc.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Conclusion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataset with no duplicates, with no outliers, and with feature selection (columns removed), we note that the Naive Bayes Classifier performs the best with a classification accuracy of 75.98%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
