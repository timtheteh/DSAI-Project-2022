{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing Essential Libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas-profiling in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (1.18.5)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.1.0 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.1.0)\n",
      "Requirement already satisfied: seaborn>=0.10.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (0.11.2)\n",
      "Requirement already satisfied: joblib~=1.0.1 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (1.0.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (3.2.2)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (1.0.5)\n",
      "Requirement already satisfied: phik>=0.11.1 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.12.2)\n",
      "Requirement already satisfied: markupsafe~=2.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (2.0.1)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.1.12)\n",
      "Requirement already satisfied: requests>=2.24.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (2.24.0)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (5.3.1)\n",
      "Requirement already satisfied: missingno>=0.4.2 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.5.1)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.4 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (0.7.4)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (1.9.0)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas-profiling) (2.11.2)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (4.64.0)\n",
      "Requirement already satisfied: multimethod>=1.4 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pandas-profiling) (1.8)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2020.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (2020.6.20)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (19.3.0)\n",
      "Requirement already satisfied: networkx>=2.4 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (2.4)\n",
      "Requirement already satisfied: Pillow; extra == \"type_image_path\" in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (7.2.0)\n",
      "Requirement already satisfied: imagehash; extra == \"type_image_path\" in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (4.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cyx_9\\appdata\\roaming\\python\\python38\\site-packages (from pydantic>=1.8.1->pandas-profiling) (4.1.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from tqdm>=4.48.2->pandas-profiling) (0.4.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas-profiling) (1.15.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from networkx>=2.4->visions[type_image_path]==0.7.4->pandas-profiling) (4.4.2)\n",
      "Requirement already satisfied: PyWavelets in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.7.4->pandas-profiling) (1.1.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (5.7.0)\n",
      "Requirement already satisfied: six in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from plotly) (1.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: catboost in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: graphviz in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (0.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (1.5.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (1.0.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (3.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (5.7.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from catboost) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\cyx_9\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas-profiling\n",
    "!{sys.executable} -m pip install plotly\n",
    "!{sys.executable} -m pip  install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing various models and tools \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the dataset that has been cleaned and does not contain any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.66         0.00             1.8      0.075   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 13.0                  40.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        0  \n",
       "1      9.8        0  \n",
       "2      9.8        0  \n",
       "3      9.8        1  \n",
       "4      9.4        0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data = pd.read_csv(\"binned_no_dup.csv\")\n",
    "\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Machine Learning</h1>\n",
    "\n",
    "We split the dataset into the predictors (X) and the response variable 'quality' (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = 'quality'\n",
    "X = wine_data.drop([target_label], axis=1)\n",
    "y = wine_data[target_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x2545884aca0>,\n",
       "  <matplotlib.axis.XTick at 0x2545884abb0>],\n",
       " <a list of 2 Text major ticklabel objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x = y.value_counts().index.tolist(),\n",
    "       height = y.value_counts())\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train Test Split </h2>\n",
    "\n",
    "We use the train_test_split() function to split the dataset into the train set, and the test set, with a test size of 0.2.\n",
    "We also specify stratify = y. This is to ensure that the ratio of good:bad values are consistent throughout the train set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Fitting & Hyperparameter Tuning</h2>\n",
    "\n",
    "For each of the models used below, the GridSearchCV function is used to determine the optimal hyperparameters for each model (i.e. the hyperparameters which result in the highest accuracy scores on the trainset). 5-fold cross validation is used to obtain unbiased results as well as to determine the optimal hyperparameters of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries used for model fitting\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> (1) Logistic Regression </H2>\n",
    "\n",
    "Logistic regression is a process of modeling the probability of a discrete outcome given an input variable. Logistic regression is a useful analysis method for classification problems, where you are trying to determine if a new sample fits best into a category. \n",
    "It is a simple and more efficient method for binary and linear classification problems. It performs relatively well in our dataset as our response variables are binary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 10.0, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear'}\n",
      "Best CV score:  0.7433306557307742\n",
      "Test accuracy:  0.7352941176470589\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'penalty':['l1', 'l2', 'elasticnet'],\n",
    "              'C': np.logspace(-3, 3, 7),\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "             'random_state': [42]}\n",
    "\n",
    "grid_search_logreg = GridSearchCV(LogisticRegression(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=1)\n",
    "grid_search_logreg.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_logreg.best_params_)\n",
    "print(\"Best CV score: \", grid_search_logreg.best_score_)\n",
    "\n",
    "best_logreg = LogisticRegression(**grid_search_logreg.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (2) Gaussian Naive Bayes </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. \n",
    "\n",
    "For our dataset, we chose to use Gaussian Naive Bayes where the likelihood of each predictor is assumed to be Gaussian (normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'var_smoothing': 2.310129700083158e-09}\n",
      "Best CV score:  0.7230668414154653\n",
      "Test accuracy:  0.7279411764705882\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'var_smoothing': np.logspace(0,-9, num=100),}\n",
    "\n",
    "grid_search_gnb = GridSearchCV(GaussianNB(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "grid_search_gnb.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_gnb.best_params_)\n",
    "print(\"Best CV score: \", grid_search_gnb.best_score_)\n",
    "\n",
    "best_gnb = GaussianNB(**grid_search_gnb.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_gnb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (3) Decision Tree Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree Classifier belongs to the family of supervised learning algorithms. The intuition behind Decision Trees is that you use the dataset features to create yes/no questions and continually split the dataset until you isolate all data points belonging to each class. The Decision Tree classifier can handle both categorical and numerical data (numerical data as predictors in this dataset), and is simple to understand and interpret since the trees can be visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 1, 'random_state': 42}\n",
      "Best CV score:  0.7147888217139475\n",
      "Test accuracy:  0.6691176470588235\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth':[i for i in range(1,50,1)],\n",
    "             'random_state': [42]}\n",
    "\n",
    "grid_search_dtc = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_dtc.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_dtc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_dtc.best_score_)\n",
    "\n",
    "best_dtc = DecisionTreeClassifier(**grid_search_dtc.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_dtc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (4) Random Forest Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier builds multiple decision trees and merges them together to get a more accurate and stable prediction. One big advantage of random forest is that it can be used for both classification and regression problems, and is suitable for binary classification of wines in this dataset.  Random forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in an improved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 7, 'n_estimators': 410, 'random_state': 42}\n",
      "Best CV score:  0.7534393100240984\n",
      "Test accuracy:  0.7757352941176471\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth':[i for i in range(1,30,3)],\n",
    "             'n_estimators': [j for j in range(10, 601, 40)],\n",
    "             'random_state': [42]}\n",
    "\n",
    "\n",
    "grid_search_rfc = GridSearchCV(RandomForestClassifier(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_rfc.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_rfc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_rfc.best_score_)\n",
    "\n",
    "best_rfc = RandomForestClassifier(**grid_search_rfc.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (5) AdaBoost Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get a high accuracy strong classifier. The method automatically adjusts its parameters to the data based on the actual performance in the current iteration, meaning, both the weights for re-weighting the data and the weights for the final aggregation are re-computed iteratively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.1, 'n_estimators': 50, 'random_state': 42}\n",
      "Best CV score:  0.7488436984737665\n",
      "Test accuracy:  0.7463235294117647\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'learning_rate':[0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "             'n_estimators': [j for j in range(10, 601, 40)],\n",
    "             'random_state': [42]}\n",
    "\n",
    "grid_search_abc = GridSearchCV(AdaBoostClassifier(), param_grid, cv= KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_abc.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_abc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_abc.best_score_)\n",
    "\n",
    "best_abc = AdaBoostClassifier(**grid_search_abc.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_abc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (6) CatBoost Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CatBoost Classifier is a high-performance open source library for gradient boosting on decision trees. In the growing procedure of the decision trees, CatBoost does not follow similar gradient boosting models. Instead, CatBoost grows oblivious trees, which means that the trees are grown by imposing the rule that all nodes at the same level, test the same predictor with the same condition, and hence an index of a leaf can be calculated with bitwise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.1, 'n_estimators': 400, 'random_state': 42}\n",
      "Best CV score:  0.7424343635056864\n",
      "Test accuracy:  0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "param_grid = {'learning_rate': [0.08, 0.1, 0.15, 0.2, 0.25, 0.3], \n",
    "             'n_estimators': [200, 250, 300, 350, 400, 450, 500],\n",
    "             'random_state': [42]}\n",
    "\n",
    "\n",
    "grid_search_cbc = GridSearchCV(CatBoostClassifier(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_cbc.fit(X_train,y_train, silent=True)\n",
    "print(\"Best parameters: \", grid_search_cbc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_cbc.best_score_)\n",
    "\n",
    "best_cbc = CatBoostClassifier(**grid_search_cbc.best_params_).fit(X_train, y_train, silent=True)\n",
    "print(\"Test accuracy: \", best_cbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> (7) GradientBoost Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifiers are the AdaBoosting method combined with weighted minimization, after which the classifiers and weighted inputs are recalculated. The objective of Gradient Boosting Classifiers is to minimize the loss, or the difference between the actual class value of the training example and the predicted class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.008, 'n_estimators': 600, 'random_state': 42}\n",
      "Best CV score:  0.7479347228681352\n",
      "Test accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'learning_rate':[0.008, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24], \n",
    "             'n_estimators': [100, 150, 200, 250, 300, 350, 400, 500, 550, 600],\n",
    "             'random_state': [42]}\n",
    "\n",
    "grid_search_gbc = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=KFold(5, shuffle=True, random_state=42), n_jobs=-1)\n",
    "\n",
    "grid_search_gbc.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid_search_gbc.best_params_)\n",
    "print(\"Best CV score: \", grid_search_gbc.best_score_)\n",
    "\n",
    "best_gbc = GradientBoostingClassifier(**grid_search_gbc.best_params_).fit(X_train, y_train)\n",
    "print(\"Test accuracy: \", best_gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Conclusion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataset with no duplicates, we note that the Random Forest Classifier performs the best with a classification accuracy of 77.57%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
